|                                                                                                                                                                                                                                                                                                                           Question                                                                                                                                                                                                                                                                                                                           |                                                                                                                                                                                                                                                                                                                      Answer                                                                                                                                                                                                                                                                                                                      |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|                                                                                                                                                                                                                                                             **1. What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?**                                                                                                                                                                                                                                                             | I believe that the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances is around 20. This is because decision trees from this chapter were based on Scikit-Learn's CART algorithm, which produces only _binary trees_. Meaning there would be a `log2` depth complexity for a balanced tree. But there is a nuance. An unrestricted decision tree can grow very deep, deeper than log2(n). This may lead to overfitting and poor generalization performance. Therefore, it is important to use techniques like pruning or setting a maximum depth to control the tree's complexity. |
|                                                                                                                                                                                                                                                         **2. Is a node's Gini impurity generally lower or higher than its parent's? Is it _generally_ lower/higher or is it _always_ lower/higher?**                                                                                                                                                                                                                                                         |                                                                                                                                 It is _generally_ lower, because CART algorithm chooses splits that **maximize the reduction in impurity**. At each step, the algorithm selects the feature and threshold that will create the most homogeneous (pure) subsets. It is not _always_ lower because, if a split doesn't improve the purity of the classes, a child node might have the same impurity as the parent.                                                                                                                                 |
|                                                                                                                                                                                                                                                                         **3. If a decision tree is overfitting the training set, is it a good idea to try decreasing `max_depth`?**                                                                                                                                                                                                                                                                          |                                                  To avoid overfitting the training data, we do need to restrict decision tree's freedom during training. There are different hyperparameters that may be regularized, but we may at the very least restrict `max_depth`. Reducing it will regularize the model and thus reduce the risk of the tree becoming too complex and essentially "memorize" the training data rather than learning generalizable patterns. Other hyperparameters like `min_samples_split` or `min_samples_leaf` can also be adjusted to control the tree's complexity.                                                   |
|                                                                                                                                                                                                                                                                       **4. If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?**                                                                                                                                                                                                                                                                       |                                                                                                           Decision trees require very little preparation. They don't require feature scaling at all. If your model is underfitting, you may want to consider: a) increasing its complexity (for example increase its `max_depth`, decrease `min_samples_split`, `min_samples_leaf`), b) adding more informative features / feature engineering, c) use more powerfull tree-based models like Random Forests or Gradient Boosted Trees.                                                                                                           |
|                                                                                                                                                                                   **5. If it takes one hour to train a decision tree on a training set containing one million instances, roughly how much time will it take to train another decision tree on a training set containing ten million instances? Hint: consider CART's algorithm computational complexity.**                                                                                                                                                                                   |                                                                                                                                         Finding the optimal tree is known to be _NP-complete_ problem. It requires roughly `O(exp(m))` time, to be more precise - `O(n x m x log(n))`, where `n` is the number of instances and `m` is the number of features. For training set of 10M it will take `10 x log(10) / loag(1) = 10 x 1.33 ~= 13.3` times longer. So it should approximately take 13 hours.                                                                                                                                         |
|                                                                                                                                                                                                                                                  **6. If it takes one hour to train a decision tree on a given training set, roughly how much time will it take you if you double the number of features?**                                                                                                                                                                                                                                                  |                                                                                                                                                                                     Time complexity of the CART algorithm is `O(n x m x log(n))`, where `n` is the number of instances and `m` is the number of features. If you double the number of features, the time complexity will increase by a factor of 2. So it should approximately take 2 hours.                                                                                                                                                                                     |
| **7. Train and fine-tune a decision tree for the moons dataset by following these steps:** <br><br> a. Use `make_moons(n_samples=10000, noise=0.4)` to generate a moons dataset.<br> b. Use `test_train_split()` to split the dataset into a training set and a test set.<br> c. Use grid search with cross-validation (with the help of the `GridSearchCV` class) to find good hyperparameter values for a `DecisionTreeClassifier`. Hint: try various values for `max_leaf_nodes`.<br> d. Train it on the full training set using these hyperparameters, and measure your model's performance on the test set. You should get roughly 85% to 87% accuracy. |                                                                                                                                                                                                                                                                                                                    Check the solution [here](./ex07/main.py).                                                                                                                                                                                                                                                                                                                    |
| **8. Grow a forrest by following these steps:** <br><br> a. Continuing the previous exercise, generate 1000 subsets of the training set, each containing 100 instances selected randomly. Hint: you can use Scikit-Learn's `ShufffleSplit` class for this<br> b. Train one decision tree on each subset, using the best hyperparameters' values found in the previous exercise. Evaluate these 1000 decision trees on the test set. Since they were trained on smaller sets, these decision trees will likely perform worse than the first decision tree, achieving only about 80% accuracy.<br> c. Now comes the magic. For each test set instance, generate the predictions of the 1000 decision trees and keep only the most frequent prediction (you can use SciPy's `mode` function for this). This approach gives you _majority-vote-predictions_ over the test set.<br> d. Evaluate thesee predictions on the test set: you should obtain a slightly higher accuracy than your first model (between 0.5 and 1.5 % improvement). Congratulations, you have trained a random forrest classifier. |                                                                                                                                                                                                                                                                                                                    Check the solution [here](./ex08/main.py).                                                                                                                                                                                                                                                                                                                    |
