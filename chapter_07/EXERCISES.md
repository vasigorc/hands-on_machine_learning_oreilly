|                                                                                                                                                                                                                                                                                                    Question                                                                                                                                                                                                                                                                                                    |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|                                                                                                                                                                                           **1. If you have trained five differet models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?**                                                                                                                                                                                           |                                                                                                                                                                                                                                                                      I believe that there is a chance to improve the score, key factor to this being the diversity in both errors that each model committed, as well as the potential model types or hyperparameter values. _Random Forests_ technique doesn't apply in this scenario, since the requirement is strict about the data being the same. Instead, even a _Voting Classifier_ could produce a better result - while each estimator had the same score overall, their accuracy could had been achieved on different data instances, a _Voting Classifier_ could aggregate these predictions. Another technique that we could try is _Boosting_, which trains different predictors, each trying to correct its predecessor on the same training data. Each new predictor (except for the first one) is trained using the updated weights. The algorithm may stop when a specified number of predictors is reached, or when a perfect predictor is found. This approach is called _Ada Boosting_. A somewhat similar approach is called _Gradient Boosting_ - instead of tweaking instance weights it tries to fit each new predictor on the _residual errors_, at each step ensemble's predictions gradually getting better. In the given scenario we may just request to run all five predictors, or, more generally, try to tweak `learning_rate` and `n_iter_no_change` hyperparameters to control overffitting or underfitting. _Histogram-Based Gradient Boosting_ is another flavor of the same approach, which also grants the benefits of reduced possible thresholds, and a better optimized computational complexity. The last ensemble method that we could use is _Stacking_: it is similar to a _Voting Classifier_, but instead of requiring a defninite aggregate function (e.g. _hard voting_) it trains another model one layer up to identify the optional function.                                                                                                                                                                                                                                                                       |
|                                                                                                                                                                                                                                                                    **2. What is the difference between hard and soft voting classifiers?**                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Both of these classifiers are just different aggregation functions used in _Voting Classifiers_. A _hard voting_ classifier stands for the majority-vote approach, and it is the default function. When you call the voting classifier's `predict()` method, it analyzes which class was selected for the given input by the majority of its predictors. When all nested classifiers can also estimate class probabilities (they implement `predict_proba()`), then we may ask for the highest class probability, avereged over all individual classifiers. This is called _soft voting_ and it often yields better performance, since it gives more weight to highly confident votes (and not just an increment in popularity). One would pick _soft validator_ when all classifiers can estimate probabilities and they are well-calibrated (their inferred probability maps accurately to the actual likelihood), or prefer _hard voting_ otherwise.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 |
|                                                                                                                                                                                                     **3. Is it possible to speed-up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forrests, or stacking ensembles?**                                                                                                                                                                                                     | I think that, from the point of view of parallellization and distributivity, ensemble methods can be divided into three categories: a) those which can be parallelized, b) those which cannot, and c) those which can, but with a spin of some design complexity. To the first category, I attribute any flavor of _Bagging_ and _Pasting_ aglorithms, of which _Random Forrests_ I reckon to be one, - they act by training individual predictors on different random subsets of the training set. In this way multiple CPUs are an option, and thus, so are multiple servers (Scikit-Learn even ships with the `n_jobs` parameter which is just the parallelization level, defaulting to `-1`, to use all available cores). In practice this can implemented for big datasets ussing plethora of tools, ranging from AWS SageMaker's XGBoost, via Kubeflow Pipelines, to AWS Batch with Lambdas. All _Boosting_ methods I would categorize as being non-parallelizable, simply because they are sequential by nature. But a parenthesis is worth to be inserted here: there are optimized implementations of gradient boosting, such as the afore mention XGBoost, CatBoost, and LightGBM, which among other features, provide possibility for GPU acceleration, which for me qualifies as parallelization. Finally, I classified _Stacking_ as having some design complexity for parallel implementation. While training base models could be parallelized (depending on the ensemble methods used for individual estimators), the _blending_ phase should use the outputs of all those individual estimators, however the _blending_ (meta-model training) stage is typically much smaller and computationally lighter than training the individual models. So it sounds like a Map-Reduce or merge sort problem, which is doable, note that once the blender is trained, the base predictors have to be retrained with the full original testing set. To make things even more complicated, sometimes you may want to use several blenders, and then have a top blender at an extra layer to produce the final prediction. It is for this outward complexity that my first intuition would be to use vertical scaling rather than horizontal one for this ensemble method. But, in all fairness, once the meta-model is trained, prediction can be very efficient as all models can make predictions in parallel. |
|                                                                                                                                                                                                                                                                              **4. What is the benefit of out-of-bag evaluation?**                                                                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Out-Of-Bag evaluation is made possible due to how `BaggingClassifier` is implemented. About 36% of training instances are not sampled for each estimator which allows to use those instances as the validation set. Simply by setting `oob_score=True` on the estimator, we can then get a pretty accurate accuracy score for the entire training set just by invoking the `fit` method of the classifier. Typically this score reflects the likely score that the model will be able to achieve on the test set. Another advantage of this technique is the computational efficiency - it removes the necessity for additional cross-validation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
|                                                                                                                                                                                                             **5. What makes extra-trees ensembles more random than random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?**                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The random forest algorithm introduces extra randomness when growing trees, instead of searching for the very best feature when splitting a node, it searches for the best feature among a random subset of features. By default it samples âˆšn features (where _n_ is the total number of features). With _Extra-Trees_ it is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular decision trees do). With Scikit-Learn, in order to enable this feature for a `DecisionTreeClassifier`, it suffices to set `splitter="random"`. Extra-Tree classifiers are much faster to train, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of a growing tree. Note that it is not guaranteed that an `ExtraTreeClassifier` will perform better than `RandomForrestClassifier` - the safest approach is to try and compare both using cross-validation.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                                                                                                                                                                                                                                 **6. If your AdaBoost ensemble underfits the training set, should you increase or decrease the learing rate?**                                                                                                                                                                                                                                                 |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              One way for a new predictor to correct its predecessor in _AdaBoost_ is to pay a bit more attention to the training instances that the predeccors underfit. This results in new predictors focusing more and more on the hard cases. _Ada Boost_ increases the relative weight of misclassified training instances, and trains the subsequent estimator using the updated weights. One way to control instance weights is via _learning rate_ hyperparameter. This parameter controls the size of steps. From this point of view _AdaBoost_ has some similarities with _Gradient Descent_. If _AdaBoost_ underfits, one should increase the learning rate. This makes each individual estimator's contribution bigger and the algorithm focuses more strongly on misclassified examples. A bigger learning rate leads to less steps before converging, and so another hyperparameter change that _might_ be required is to increasse the number of estimators. These two changes should result model's increased ability to fit training data and capture the patterns in the training data that it was previously missing.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |
|                                                                                                                                                                                                                                            **7. If your gradient boosting ensemble overfits the training set, should you increase or decrease the learning rate?**                                                                                                                                                                                                                                             |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    The `learning_rate` hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.05, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization techniques called _shrinkage_. If a GBRT overfits the trainiing set, one should decrease the `learning_rate`. Equally, to avoid overfitting, you could perform a cross-validation using `GridSearchCV` or `RandomizedSearchCV`. And with GBRT there is a even a simpler way: set the hyperparameter `n_iter_no_change` to some value (e.g. 10), then the `GradientBoostingRegressor` will automatically stop adding trees if it sees that the last 10 trees didn't help (similar to early stopping with _Gradient Descent_s). \_n_iter_no_change_, under the hood, makes a 10% (default, cofigurable with `validation_fraction` parameter) validation set split which allows to re-evaluate the model upon adding a new tree. The `tol` hyperparameter determines thee maximum performance improvement that counts as negligible. Note that, when setting `n_iter_no_change` too low training may stop too soon and the model will underfit.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |
| **8. Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g. use 50*000 instances for training, 10_000 each for validation and testing). Then train the various classifiers, such as a random forest classifier, an extra-treees classifier, and a SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using \_soft* or _hard_ voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     To be provided                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |
