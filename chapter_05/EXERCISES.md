|                                                                                                                                                                                                                                         Question                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        Answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
| :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|                                                                                                                                                                                                            **1. What is the fundamental idea behind support vector machines?**                                                                                                                                                                                                            |                                                                                                                                                                                                                                         Support Vector Machines (SVMs) are based on finding a decision boundary (a hyperplane) that best separates different classes. SVMs maximize the margin, which is the distance between the decision boundary and the closest data points from each class, called support vectors. This margin is "the street" that separates the classes. SVMs can handle both linearly and non-linearly separable data by using kernels to transform the data into higher-dimensional spaces. In soft-margin SVMs, some margin violations or misclassifications are allowed to balance model complexity and accuracy.                                                                                                                                                                                                                                         |
|                                                                                                                                                                                                                             **2. What is a support vector?**                                                                                                                                                                                                                              |                                                                                                                                                                                                                                                                              A support vector is a data instance that lies closest to the decision boundary in an SVM model. These points are critical because they determine the position and orientation of the decision boundary. In soft-margin SVMs, support vectors can lie within the margin or even violate it slightly. The "support" comes from the fact that these points define and "support" the margin, and removing or changing them can alter the decision boundary. Data points father away from the margin do not influence the model.                                                                                                                                                                                                                                                                              |
|                                                                                                                                                                                                              **3. Why is it important to scale the inputs when using SVMs?**                                                                                                                                                                                                              |                                                                                                                                                                                                                                                         SVMs are sensitive to the scale of input features because they rely on distance calculations, such as Euclidean distance, and dot products to determine the decision boundary and margins. If features are not scaled, those with larger numerical ranges can disproportionately influence the model, leading to a skewed decision boundary. This is especially critical when using kernel functions, as improper scaling can distort the mapping to higher-dimensional spaces. Scaling ensures that all features contribute equally, and improves model performance.                                                                                                                                                                                                                                                         |
|                                                                                                                                                                                      **4. Can a SVM classifier output a confidence score when it classifies an instance? What about a probability?**                                                                                                                                                                                      |                                                                                                                                                                                        SVM classifiers use decision scores to make predictions. These scores represent the signed distance from each instance to the decision boundary, which can serve as a confidence measure but are not probabilities. SVMs do not directly estimate class probabilities. To obtain probabilities, one can use Platt scaling, which involves cross-validation to generate out-of-sample predictions for each instance in the training set and then fitting a LogisticRegression model to map decision scores to probabilities. This process can slow down the model considerably. Alternatively, some implementations, such as scikit-learn's SVC with probability=True, perform this step automatically.                                                                                                                                                                                         |
|                                                                                                                                                                                                        **5. How can you choose between `LinearSVC`, `SVC`, and `SGDClassifier`?**                                                                                                                                                                                                         | All three are SVM-related classifiers, each suited to different purposes and performance trade-offs. `LinearSVC` is designed for linear datasets and does not support the kernel trick. It is best used when you are confident about the linear nature of your dataset and need a fast solution, as its training complexity is _O(m Ã— n)_. `SVC`, on the other hand, supports the kernel trick, making it suitable for non-linear datasets. However, its computational complexity ranges between _O(mÂ² Ã— n)_ and _O(mÂ³ Ã— n)_, which limits its scalability to small or medium-sized datasets. It excels in handling sparse datasets with many features. Finally, `SGDClassifier` is another option for linear datasets but relies on stochastic gradient descent. This allows it to perform incremental learning with low memory usage, making it ideal for large datasets that do not fit in RAM (out-of-core learning). It also has a computational complexity of _O(m Ã— n)_. All three algorithms require feature scaling, so it is important to preprocess your data accordingly. |
|                                                                                                                                                            **6. Say you've trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease `gamma`? What about `C`?**                                                                                                                                                            |                                                                                                                                                                                                                                                                                                                                                                                                                                                               Î³ (`gamma`) acts like a regularization hyperparameter. In the case that it underfits one should increase it, the same applies to the `C` hyperparameter.                                                                                                                                                                                                                                                                                                                                                                                                                                                                |
|                                                                                                                                                                                                                 **7. What does it mean for a model to be Îµ-insensitive?**                                                                                                                                                                                                                 |                                                                                                                                                                                                               In SVM regression, _Îµ_ (epsilon) is a hyperparameter margin limits of tolerance around regression line (or hyperplane). A model is said to be Îµ-insensitive when it ignores the errors within this margin: data points that fall within the Îµ margin do not contribute to the loss function. Reducing _Îµ_ narrows the margin, increasing the number of support vectors.This may regularizes the model, but may as well lead to overfitting it. Conversely, increasing Îµ broadens the margin, leading to a simpler model with fewer support vectors. If new training instances fall withing the margin, they do not affect predictions made by the model.                                                                                                                                                                                                                |
|                                                                                                                                                                                                                    **8. What is the point of using the kernel trick?**                                                                                                                                                                                                                    |                                                                                                             In Machine Learning, a _kernel_ is a function that computes the dot product of two vectors in a high-dimensional feature space without explicitly performing the transformation Ï•. This is known as the kernel trick, and it allows algorithms to implicitly work in higher-dimensional spaces without the computational cost of explicitly adding features. For example, in SVMs, the kernel trick enables the model to separate data that is not linearly separable in the original feature space by implicitly transforming it into a higher-dimensional space where a linear separation becomes possible. Different types of kernels, such as polynomial and radial basis function (RBF), can be used to capture different kinds of relationships between data points. This approach makes the model more powerful while keeping computations efficient.                                                                                                              |
|                                                                                                                                                   **9. Train a `LinearSVC` on a linearly separable dataset. Then train an `SVC` and a `SGDClassifier` on the same dataset. See if you can get them to produce roughly the same model.**                                                                                                                                                   |                                                                                                                                                                                                                                                                                                                                                                                                      Check [ex09/main.py](ex09/main.py) We implemented three different classifiers on a linearly separable dataset created using `make_classification`. Below are the decision boundaries produced by each classifier. Please refer to images under this table for more details.                                                                                                                                                                                                                                                                                                                                                                                                      |
| **10. Train an SVM classifier on the wine dataset, which you can load using `sklearn.dataset.load_wine`. This dataset contains the chemical analyses of 178 wine samples produced by 3 different cultivators: the goal is to train a classification model capable of predicting the cultivator based on the wine's chemical analysis. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all three classifiers. What accuracy can you reach?** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    Check [ex10/main.py](./ex10/main.py) file for final implementation, and [this markdown file](./ex10/exercise_10_.md) for a copy of the original Jupyter Notebook, with all investigation, validation, fine-tuning and testing results.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |

**LinearSVC:**
![LinearSVC Decision Boundary](ex09/LinearSVC%20Decision%20Boundary.png)

**SVC with linear kernel:**
![SVC Decision Boundary](ex09/SVC%20Decision%20Boundary.png)

**SGDClassifier with hinge loss:**
![SGDClassifier Decision Boundary](ex09/SGDClassifier%20Decision%20Boundary.png)

All three classifiers produced similar decision boundaries, which is expected since:

1. The dataset is linearly separable
2. All three implement linear classification (LinearSVC directly, SVC with linear kernel, SGDClassifier with hinge loss)
3. Parameters were tuned to achieve similar margins:
   - LinearSVC and SVC used C=1
   - SGDClassifier used alpha=0.001 (roughly equivalent to 1/C)

The visualizations show:

- Colored regions representing class predictions
- Solid black line showing the decision boundary
- Dashed lines showing the margins ("street" between classes)
- Data points colored by their true class labels

