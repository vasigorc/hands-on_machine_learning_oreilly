|                                                                                                                                                                                                                                                                                                                                                                                                                                                         Question                                                                                                                                                                                                                                                                                                                                                                                                                                                         |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             Answer                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |
| :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: | :-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------: |
|                                                                                                                                                                                                                                                                                                                                                                                                                    **1. How would you define clustering? Can you name a few clustering algorithms?**                                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                                                                                                                                                                                                                       _Clustering_ is the task of identifying similar instances and assigning them to _clusters_, or groups of similar instances. Clustering is used in a wide variety of applications, including: _Customer segmentation_, _Data analysis_, _Dimensionality reduction_. The are many clustering algorithms, a few of the more popular ones are _k-means_, _DBSCAN_, _Gaussian Mixture_.                                                                                                                                                                                                                                                                                                                        |
|                                                                                                                                                                                                                                                                                                                                                                                                                         **2. What are some of the main applications of clustering algorithms?**                                                                                                                                                                                                                                                                                                                                                                                                                          |                                                                                                                                                                                                                                                                                                          Clustering is used in a wide variety of applications, including: customer segmentation (market analyris), data analysis, dimensionality reduction, feature engineering (cluster affinities can be used as extra-features), anomaly detection (every instance that has low affinity to all clusters is likely an anomaly), semi-supervised learning (label propagation), search engines, image segmentation .                                                                                                                                                                                                                                                                                                           |
|                                                                                                                                                                                                                                                                                                                                                                                                               **3. Describe two techniques to select the right number of clusters when using _k-means_?**                                                                                                                                                                                                                                                                                                                                                                                                                | One, rather coarse, way is to plot a function of _k_ (number of clusters) to a corresponding _inertia_. Inertia will normally drop very quickly as k increases, but only to a certain point where the graph flattens a little. The resulting function looks like an arm, and the point from where the drop becomes more gradual resembles an elbow. This elbow area is where one will likely find the optimal number of clusters. A more precise (but also more computationally expensive) approach is to use the _silhouette score_), which is the mean _silhouette coefficient_ over all the instances. An instance's silhouette coefficient is equal to _(b - a) / max(a, b)_ where, _a_ is the mean distance to the other instances in the same cluster and _b_ is the mean nearest-cluster distance. This value will vary betweeen -1 and +1, a coefficient close to +1 means that the instance is well inside its own cluster, 0 instance is at a cluster boundary, and -1 means instance is assigned to a wrong cluster. |
|                                                                                                                                                                                                                                                                                                                                                                                                                          **4. What is label propagation? Why would you implement it and how?**                                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                              _Label propagation_ refers to propagating known labels to the entirety of a dataset. It is one use case of clustering - semi-supervised learning: when we have a plenty of unlabeled instances and very few labeled instances. First stage of this approach is to cluster the training set into _n_ clusters representing all known lables. Then, for each cluster we'll find the instances closest to the centroid - we'll call these instances the _representative instances_. And now we are able to propagate the labels to all the other instances in the same cluster (label propagation). We could do even better by ignoring the instances that are farthest from their cluster center: this should eliminate some outliers.                                                                                                                                               |
|                                                                                                                                                                                                                                                                                                                                                                                              **5. Can you name two clustering algorithms that can scale to large datasets? And two that look for regions of high density?**                                                                                                                                                                                                                                                                                                                                                                                              |                                                                                                                    One algorithm which will consistently scale well to large number of instances is _Agglomerative clustering_, given that you will provide it a connectivity matrix, which is a sparse _m x m_ matrix indicates which pairs of instances are neighbours. Another good candidate for this category is _BIRCH_ (balanced iterative reducing and clustering using hierarchies), but with a condition that the number of features is not too large (< 20). _DBSCAN_ (density-based spatial clustering of applications with noise) defines clusters as continuous regions of high density, thus it qualifies for the second category. Yet another candidate is _Mean-shift_, which shifts the circles in the direction of higher density, until each of them has found a local density maximum.                                                                                                                     |
|                                                                                                                                                                                                                                                                                                                                                                                                          **6. Can you think of a use case where active learning would be useful? How would you implement it?**                                                                                                                                                                                                                                                                                                                                                                                                           |                                                                                                                                       _Active learning_ refers to the practice when a human expert interacts with the learning algorithm, providing labels for specific instances when the algorithm requests them. One of the most common implementations is called _uncertainty sampling_. It is a 2 step iterative process, where a model is trained on the labeled instances gathered so far and makes predictions for all unlabelled instances, then the instances for which the model has lowest probabilities are given to the expert for labelling, this continues until the performance improvements stop being considerable. Examples of usage of this implementation could be in predicting whether a manufacturing item is defective or not, or in a image recognition model.                                                                                                                                       |
|                                                                                                                                                                                                                                                                                                                                                                                                                      **7. What is the difference between anomaly detection and novelty detection?**                                                                                                                                                                                                                                                                                                                                                                                                                      |                                                                                                                                                                                                                                                                              In this chapter, Both notions were approached from the perspective of _Gaussian Mixtures_. In that context, _anomaly detection_ refers to finding any instance located in a low-density region (e.g. using Gaussian Mixtures with a density threshold). _Novelty detection_ differs from anomaly detection in that the algorithm is assumed to be trained on a "clean" dataset, uncontaminated by outliers, whereas anomaly detection does not make this assumption.                                                                                                                                                                                                                                                                               |
|                                                                                                                                                                                                                                                                                                                                                                                                                            **8. What is a Gaussian mixture? What tasks can you use it for?**                                                                                                                                                                                                                                                                                                                                                                                                                             |                                                                                                                                                                                      A _Gaussian mixture model_ (GMM) is a probabilistic model that assumes that the instances were generated from a mixture of several Gaussian distributions whose parameters are unknown. All the instances generated from a single Gaussian distribution form a cluster that typically looks like an ellipsoid. A Gaussian mixture is a _generative_ model, meaning you can sample new (cluster indexed) instances from it. It is possible to estimate the density of the model at any given location, to estimate the _probability_ that an instance will fail within a particular region. This model may be used for anomaly and novelty detection.                                                                                                                                                                                       |
|                                                                                                                                                                                                                                                                                                                                                                                                     **9. Can you name the two techniques to find the right number of clusters when using a Gaussian mixture model?**                                                                                                                                                                                                                                                                                                                                                                                                     |                                                                                                                         You may try to find the model that minimizes a _theoretical information criterion_, such as the _Bayesian information criterion_ (BIC) or the _Akaike information criterion_ (AIC). Both penalize models that have more parameters to learn (i.e. more clusters) and reward models that fit the data well. Both end-up selecting the same model. When they differ, the model selected by the BIC tends to be simpler (fewer clusters) than the one selected by AIC, but tends to not fit the data quite as well, especially with large datasets. Also, rather than manually searching for the optimal number of clusters, you can use the `BayesianGaussianMixture` class from Scikit-Learn, which is capable of giving weights equal or close to zero to unnecessary clusters.                                                                                                                         |
| **10. The classic Olivetti faces dataset contains 400 grayscale 64 X 64-pixel images of faces. Each image is flattened to a 1D vector of size 4096. Forty different people were photographed (10 times each), and the usual task is to train a model that can predict which person is represented in each picture. Loadthe dataset using the `sklearn.datasets.fetch_olivetti_faces()` function, then split it into a training set, a validation set, and a test set (note that the dataset is already scaled between 0 and 1). Since the dataset is quite small, you will probably want to use stratified sampling to ensure that there are the same number of images per person in each set. Next, cluster the images using _k_-means, and ensure that you have a good number of clusters (using one of the techniques discussed in this chapter). Visualize the clusters: do you see similar faces in each cluster?** |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         To be provided.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |
