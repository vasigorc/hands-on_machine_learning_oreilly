{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb3f5fa-cd38-46c7-99dc-5f66fa5f2487",
   "metadata": {},
   "source": [
    "# Exercise 13\n",
    "\n",
    "Some dimensionality reduction techniques can also be used for anomaly detection. For example, take the\n",
    "Olivetti faces dataset, and reduce it with PCA, preserving 99% of the variance. Then compute the reconstruction\n",
    "error for each image. Next, take some of the modified images you built in the previous exercise and\n",
    "look at their reconstruction error: notice how much larger it is. If you plot a reconstructed image,\n",
    "you will see why: it tries to reconstruct a normal face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f74799aa-3aed-4320-93c5-cdc7b52ca9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "from numpy.typing import NDArray\n",
    "\n",
    "# functions\n",
    "def determine_optimal_pca_components(\n",
    "    X_train: NDArray[np.floating], variance_threshold: float = 0.95\n",
    ") -> int:\n",
    "    \"\"\"\n",
    "    Calculate the minimum number of principal components needed to retain a specified percentage of the variance.\n",
    "\n",
    "    Args:\n",
    "        X_train: 2D array-like, shape (n_samples, n_features), the training data.\n",
    "        variance_threshold: Float between 0 and 1, the target fraction of variance to retain (default: 0.95).\n",
    "\n",
    "    Returns:\n",
    "        The minimum number of PCA components required to preserve at least the desired variance.\n",
    "    \"\"\"\n",
    "    pca = PCA().fit(X_train)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    d = np.argmax(cumsum >= variance_threshold) + 1\n",
    "    print(\n",
    "        f\"Optimal number of PCA components to keept at least variance {variance_threshold:.0%} is {d}\"\n",
    "    )\n",
    "    return d\n",
    "\n",
    "def get_modified_faces(\n",
    "    X: NDArray[np.float64], y: NDArray[np.int32]\n",
    ") -> tuple[NDArray[np.float64], NDArray[np.int32]]:\n",
    "    \"\"\"\n",
    "    In accordance with the tasks from  exercises 12 and 13,  we modify some images (e.g. rotate, flip,\n",
    "    darken)\n",
    "\n",
    "    Args:\n",
    "        X: Array of face images, each row is a flattened image of 64x64 pixels\n",
    "        y: Array of labels corresponding to the face images\n",
    "\n",
    "    Returns:\n",
    "        X_bad_faces: Array of modified face images (rotated, flipped, darkened)\n",
    "        y_bad: Array of labels for the modified images\n",
    "\n",
    "    \"\"\"\n",
    "    # Re-using author's solution for this part (from: https://colab.research.google.com/github/ageron/handson-ml3/blob/main/09_unsupervised_learning.ipynb)\n",
    "    n_rotated = 4\n",
    "    rotated = np.transpose(X[:n_rotated].reshape(-1, 64, 64), axes=[0, 2, 1])\n",
    "    rotated = rotated.reshape(-1, 64 * 64)\n",
    "    y_rotated = y[:n_rotated]\n",
    "\n",
    "    n_flipped = 3\n",
    "    flipped = X[:n_flipped].reshape(-1, 64, 64)[:, ::-1]\n",
    "    flipped = flipped.reshape(-1, 64 * 64)\n",
    "    y_flipped = y[:n_flipped]\n",
    "\n",
    "    n_darkened = 3\n",
    "    darkened = X[:n_darkened].copy()\n",
    "    darkened[:, 1:-1] *= 0.3\n",
    "    y_darkened = y[:n_darkened]\n",
    "\n",
    "    X_bad_faces = np.r_[rotated, flipped, darkened]\n",
    "    y_bad = np.concatenate([y_rotated, y_flipped, y_darkened])\n",
    "    return X_bad_faces, y_bad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5580230b-888a-4695-9012-9b282b821d1a",
   "metadata": {},
   "source": [
    "## Step 1. Fetch and split the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "06bbfcf6-9d2c-4fb0-b44d-2322a5672a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "olivetti = fetch_olivetti_faces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "099313ca-b162-4e6c-ad6c-2d6a930db7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, _, y_train, _ = train_test_split(\n",
    "    olivetti.data,\n",
    "    olivetti.target,\n",
    "    stratify=olivetti.target,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc62398c-d4c0-454c-832c-54cef5dd6f75",
   "metadata": {},
   "source": [
    "## Step 2. Reduce dataset's dimensionality with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be5e2034-d1b7-4104-babd-8b53c674164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of PCA components to keept at least variance 99% is 222\n"
     ]
    }
   ],
   "source": [
    "n_components = determine_optimal_pca_components(X_train, 0.99)\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_reduced = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbfd73d-f3c1-4d0b-b5a4-8357c6402cf0",
   "metadata": {},
   "source": [
    "## Step 3. Compute the reconstruction error for each image\n",
    "\n",
    "We can compute the reconstruction error for each image as the mean squared error (MSE) per image \n",
    "or the total squared error per image. The exercise doesn't specify, but since it's an image, we \n",
    "might compute the MSE.\n",
    "  \n",
    "We could equally use `sklearn.metrics.mean_squared_error`, but that would return us the average \n",
    "over all samples (images) and all features. Since we're asked for the error per image, we would \n",
    "have to use it in a loop, hence vectorized `np.mean(..., axis=1)` seems more efficient and straightforward.\n",
    "\n",
    "Why `axis=1`? The array `X_train` has shape `(n_samples, n_features)`. When we compute `(X_train \n",
    "X_train_reconstructed) ** 2`, we get a matrix of squared errors of the same shape. Then, `np.mean(..., \n",
    "axis=1)` takes the mean across the columns (i.e., for each row, which represents one image). So \n",
    "we get one mean squared error per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddde551-cff3-42f7-98a0-037bd61f3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reconstructed = pca.inverse_transform(X_train_reduced)\n",
    "reconstruction_errors = np.mean(np.square(X_train - X_train_reconstructed), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1543b53a-474e-4e61-9ada-f05ec0197716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train errors (AVG): 0.00019272117060609162\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train errors (AVG): {np.mean(reconstruction_errors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4eb33ee-ab1b-4888-9897-d08924c36d47",
   "metadata": {},
   "source": [
    "## Step 4. Compute the reconstruction error for modified images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8e60815-1ce5-4b91-a835-d6e677410f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a subset (10) of modified images\n",
    "X_bad_faces, y_bad_faces = get_modified_faces(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c7a75bc7-3369-4ef7-ade8-84f97246be22",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bad_faces_reconstructed = pca.inverse_transform(pca.transform(X_bad_faces))\n",
    "reconstruction_errors_modified = np.mean(np.square(X_bad_faces - X_bad_faces_reconstructed), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b141d5a7-ce6e-4382-b216-195d7926b5d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modified images' errors (AVG): 0.004383236635476351\n"
     ]
    }
   ],
   "source": [
    "print(f\"Modified images' errors (AVG): {np.mean(reconstruction_errors_modified)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b497581-6763-4ff6-921e-8238ed029f35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
